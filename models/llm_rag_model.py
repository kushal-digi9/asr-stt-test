import httpx
import logging
from typing import Optional, List, Dict, Any
from models.llm_model import LLMModel
from models.rag_model import RAGModel

logger = logging.getLogger(__name__)

class LLMRAGModel(LLMModel):
    """Extended LLM model with RAG capabilities."""
    
    def __init__(
        self,
        ollama_url: str = "http://localhost:11434",
        model_name: str = "llama3.2:1b",
        echo_mode: bool = False,
        timeout: float = 30.0,
        qdrant_url: str = "http://localhost:6333",
        enable_rag: bool = True,
        top_k_docs: int = 3
    ):
        """
        Initialize LLM with RAG support.
        
        Args:
            ollama_url: URL of Ollama service
            model_name: Name of the model
            echo_mode: Echo mode for testing
            timeout: Request timeout
            qdrant_url: URL of Qdrant service
            enable_rag: Whether to enable RAG
            top_k_docs: Number of documents to retrieve
        """
        super().__init__(ollama_url, model_name, echo_mode, timeout)
        
        self.enable_rag = enable_rag
        self.top_k_docs = top_k_docs
        
        if self.enable_rag:
            logger.info(f"üîç Initializing RAG capabilities")
            self.rag_model = RAGModel(
                qdrant_url=qdrant_url,
                top_k=top_k_docs
            )
            logger.info(f"‚úÖ RAG enabled with top_k={top_k_docs}")
        else:
            self.rag_model = None
            logger.info(f"‚ö†Ô∏è RAG disabled")
    
    async def generate_response_with_rag(
        self,
        prompt: str,
        max_tokens: int = 150,
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate response using RAG pipeline.
        
        Args:
            prompt: User query
            max_tokens: Maximum tokens to generate
            filter_dict: Optional metadata filters for retrieval
            
        Returns:
            Dict with answer and retrieved contexts
        """
        logger.info(f"üîç Generating RAG response")
        logger.info(f"üîç Query: '{prompt[:100]}{'...' if len(prompt) > 100 else ''}'")
        
        if not self.enable_rag or self.rag_model is None:
            logger.warning("‚ö†Ô∏è RAG not enabled, falling back to standard generation")
            answer = await self.generate_response(prompt, max_tokens)
            return {
                "answer": answer,
                "contexts": [],
                "rag_enabled": False
            }
        
        try:
            # Retrieve relevant documents
            logger.info(f"üîç Retrieving relevant documents...")
            retrieved_docs = self.rag_model.retrieve(
                query=prompt,
                top_k=self.top_k_docs,
                filter_dict=filter_dict
            )
            
            if not retrieved_docs:
                logger.warning("‚ö†Ô∏è No documents retrieved, using base LLM")
                answer = await self.generate_response(prompt, max_tokens)
                return {
                    "answer": answer,
                    "contexts": [],
                    "rag_enabled": True,
                    "docs_retrieved": 0
                }
            
            # Build context from retrieved documents
            context_texts = [doc["text"] for doc in retrieved_docs]
            context_str = "\n\n".join([
                f"‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ {i+1}:\n{text}"
                for i, text in enumerate(context_texts)
            ])
            
            logger.info(f"üîç Retrieved {len(retrieved_docs)} documents")
            logger.info(f"üîç Context length: {len(context_str)} characters")
            
            # Create RAG prompt
            rag_prompt = self._build_rag_prompt(prompt, context_str)
            
            # Generate response
            answer = await self.generate_response(rag_prompt, max_tokens)
            
            logger.info(f"‚úÖ RAG response generated")
            
            return {
                "answer": answer,
                "contexts": retrieved_docs,
                "rag_enabled": True,
                "docs_retrieved": len(retrieved_docs)
            }
            
        except Exception as e:
            logger.error(f"‚ùå RAG generation failed: {e}")
            # Fallback to base LLM
            answer = await self.generate_response(prompt, max_tokens)
            return {
                "answer": answer,
                "contexts": [],
                "rag_enabled": True,
                "error": str(e)
            }
    
    def _build_rag_prompt(self, query: str, context: str) -> str:
        """
        Build prompt with retrieved context.
        
        Args:
            query: User query
            context: Retrieved context documents
            
        Returns:
            Formatted prompt
        """
        prompt = f"""‡§Ü‡§™ ‡§è‡§ï ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤ ‡§∞‡§ø‡§∏‡•á‡§™‡•ç‡§∂‡§®‡§ø‡§∏‡•ç‡§ü ‡§π‡•à‡§Ç‡•§ ‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á ‡§∞‡•ã‡§ó‡•Ä ‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç‡•§

‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä:
{context}

‡§∞‡•ã‡§ó‡•Ä ‡§ï‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {query}

‡§ï‡•É‡§™‡§Ø‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§î‡§∞ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç‡•§ ‡§Ø‡§¶‡§ø ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã ‡§µ‡§ø‡§®‡§Æ‡•ç‡§∞‡§§‡§æ ‡§∏‡•á ‡§¨‡§§‡§æ‡§è‡§Ç‡•§

‡§â‡§§‡•ç‡§§‡§∞:"""
        
        return prompt
    
    def get_rag_stats(self) -> Dict[str, Any]:
        """Get RAG model statistics."""
        if not self.enable_rag or self.rag_model is None:
            return {"rag_enabled": False}
        
        try:
            collection_info = self.rag_model.get_collection_info()
            return {
                "rag_enabled": True,
                "collection_info": collection_info,
                "top_k": self.top_k_docs
            }
        except Exception as e:
            logger.error(f"‚ùå Failed to get RAG stats: {e}")
            return {"rag_enabled": True, "error": str(e)}
